#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 23 01:30:38 2017

LgNetModelGen.py
``````````
This code reads the data generated by "Data acquisition and cleaning.py" code and uses 'btl' library to create a "Similarity Matrix" of the documents contained in the corpus based on an algebric aggregation of topic model and citation network of the corpus. Here the SQLite3 database and data sets generated by X are being used to provide the necessary documents to form the topic model and citation network.

Change this as necessary for your own database, or form the corpus from a directory of files or a list of strings using the various helper functions in corpus.py.
"""
import os 
import sys
import corpus_f   
import topicmodel_f
import btl_f          #pip install lda
import numpy as np
import scipy
import json
import csv
sys.path.append("./lib") 

import sqlite3     # Part of standard library, used in from_database()

"""
Helper functions
"""
def save_sparse(filename, sparse):
        np.savez(filename, 
                data=sparse.data, 
                indices=sparse.indices, 
                indptr=sparse.indptr,
                shape=sparse.shape
        );

def load_sparse(filename):
        loader = np.load(filename);

        return scipy.sparse.csc_matrix(
                (loader["data"], loader["indices"], loader["indptr"]), 
                shape=loader["shape"]
        );

def save_to (listORdict,name,path):     
    workingdatapath=os.path.join(path, name)
    data = open(workingdatapath, 'w')
    json.dump(listORdict, data)


def load_from(name,path):
    with open (os.path.join(path, name)) as f:
        return json.load(f)
            
"""
The example will process an entire corpus all in one run.
You can stop the process at any point or load data from
a file and begin at any point.
"""

# Create a new tokenizer
tokenizer = corpus.Tokenizer(
        stop_tokens = ["l","stat","pub"],

        use_lexical_smoothing = True,        
        use_stemming          = True,
        use_pos_tagging       = True
);
              
"""setting working dir path and other env veriables"""
os.chdir(os.getcwd())
reload(sys)
sys.setdefaultencoding("utf-8")

# Dictionary to read data from 
db_path_input = "../"+"0-Data aquisation and cleaning" #NEW-generated by faraz code
data_sets_directory_input = "../"+"0-Data aquisation and cleaning/Code's data lake/1946-2005 in_the_network extracted in the text format-no citation-from no citation num.txt files folder"

# Create a dictionary and DTM from a database 
db_path_output =  "../"+"1-LgNetModel";

# Create the citation matrix
in_network_SCOTUS_timespan_opinions_list_z=load_from ("in_network_SCOTUS_timespan_opinions_list_z",db_path+"/Code's data lake/output data")
db_title_list=in_network_SCOTUS_timespan_opinions_list_z
in_network_SCOTUS_timespan_opinions_count=len(in_network_SCOTUS_timespan_opinions_list_z)

conn = sqlite3.connect(db_path+"/SCOTUS_citation_network_in_the_time_span.db");
conn.text_factory = bytes; # Process non-ASCII characters
db = conn.cursor(); 


M_cite = scipy.sparse.lil_matrix((in_network_SCOTUS_timespan_opinions_count, in_network_SCOTUS_timespan_opinions_count), dtype=np.int8);

db.execute("""
       SELECT "ID-citing", "ID-cited"
       FROM Citations
""");  

    
for row in db:
    M_cite[int(row[0]-1),int(row[1])-1] = 1;

    
CITE= M_cite.tocsr();
save_sparse("scotus.sparse.cite", CITE);


# Create the THETA and PHI matrices from the LDA topic modeler
"""
db_title_list=[]           
for row in db:
    db_title_list.append(row[1])   #it may be wrong considering the ones that are just cited and not citing
"""    
    
(dictionary, DTM),DTM_title_list_z = corpus_faraz.from_directory(tokenizer.tokenize, data_sets_directory_input,in_network_SCOTUS_timespan_opinions_list_z);

save_to (DTM_title_list_z,"DTM_title_list_z",os.getcwd())
R_DTM_title_list=load_from ("DTM_title_list_z",os.getcwd())

dictionary.save("scotus.dictionary");
save_sparse("scotus.sparse.dtm", DTM);


(THETA, PHI) = topicmodel.lda(DTM, num_topics=100, num_passes=10);

np.save("scotus.theta", THETA);
np.save("scotus.phi", PHI);

# Create the similarity matrix
SIM = btl.similarity_matrix(THETA, M_T=10, M_O=10);
save_sparse("scotus.sparse.sim", SIM, compressed=False);


# Create the transition matrix 
TRAN = btl.weighted_transition_matrix(SIM, CITE, 0.33, 0.33, 0.33);
save_sparse("scotus.sparse.tran", TRAN);

# Create the rank matrix 
RANK = btl.rank_matrix(TRAN, 1/3.0);
save_sparse("scotus.sparse.rank", RANK);


# Create the final distance matrix 
n = RANK.shape[0] if hasattr(RANK, "shape") else len(RANK);
p=2
p_inv = 1/1.0*p;

R=RANK.toarray()
DIST = None
for i in range(n):
    row = RANK[i,:];
    tile_i = np.tile(row, [n,1]);    # Make a new matrix where every row is equal to @row
    t=np.concatenate(np.array([tile_i[ii,0].toarray() for ii in range(n)]), axis=0) #change the title_i's format to be compatable with R
    dist_i = np.sum(abs(R - t)**p, axis=1) ** p_inv;           # Compute the p-norm for R(x,y) - R(i,z), for x,y,z in n. 
    if DIST is None:
        DIST = dist_i;
    else: 
        DIST = np.concatenate([DIST, dist_i]);

DIST_Matrix_Rank=in_network_SCOTUS_timespan_opinions_count
DIST_matrix=np.reshape(DIST, (DIST_Matrix_Rank, DIST_Matrix_Rank))
DIST_sparse=scipy.sparse.csc_matrix(DIST_matrix)   
save_sparse("scotus.sparse.dist", DIST_sparse);


# Save all the model data 
dictionary.save("scotus.dictionary");
save_sparse("scotus.sparse.dtm", DTM);
np.save("scotus.theta", THETA);
np.save("scotus.phi", PHI);
save_sparse("scotus.sparse.sim", SIM);
save_sparse("scotus.sparse.cite", CITE);
save_sparse("scotus.sparse.tran", TRAN);
save_sparse("scotus.sparse.rank", RANK);
save_sparse("scotus.sparse.dist", DIST_sparse);

R_THETA=np.load("scotus.theta.npy");
np.savetxt("DIST.csv", DIST_matrix, delimiter=",")

np.savetxt("SIM.csv", SIM.toarray(), delimiter=",")

conn = sqlite3.connect(db_path+"/SCOTUS_citation_network_in_the_time_span.db");
conn.text_factory = bytes; # Process non-ASCII characters
dbb = conn.cursor(); 

dbb.execute("""
       SELECT "ID", "courtlistener_ID"
       FROM Nodes
""");  

DB_labeling={}
inverse_DB_labeling={} 
ID_labellist=[] 
Name_labellist=[]  
rows=[]
for row in dbb:
    DB_labeling[row[0]]=row[1]
    inverse_DB_labeling[row[1]]=row[0]
    ID_labellist.append(row[0])
    Name_labellist.append(row[1])
    rows.append(row)

    
#Textual Distance Matrix calculator:    
Textual_TRAN = btl.weighted_transition_matrix(SIM, CITE, 1, 0, 0);

# Create the rank matrix 
Textual_RANK = btl.rank_matrix(Textual_TRAN , 1/3.0);


# Create the final distance matrix 
# Number of rows/cols in the matrix Textual_R.
n = Textual_RANK.shape[0] if hasattr(Textual_RANK, "shape") else len(Textual_RANK);
p=2
p_inv = 1/1.0*p;


Textual_R=Textual_RANK.toarray()
Textual_SIM = None
for i in range(n):
    row = Textual_RANK[i,:];
    tile_i = np.tile(row, [n,1]);    # Make a new matrix where every row is equal to @row
    t=np.concatenate(np.array([tile_i[ii,0].toarray() for ii in range(n)]), axis=0) #change the tile_i's format to be compatable with Textual_R
    sim_i = np.sum(abs(Textual_R - t)**p, axis=1) ** p_inv;           # Compute the p-norm for Textual_R(x,y) - Textual_R(i,z), for x,y,z in n. 
    if Textual_SIM is None:
        Textual_SIM = sim_i;
    else: 
        Textual_SIM = np.concatenate([Textual_SIM, sim_i]);


Textual_SIM_Matrix_Rank=in_network_SCOTUS_timespan_opinions_count
Textual_SIM_matrix=np.reshape(Textual_SIM, (Textual_SIM_Matrix_Rank, Textual_SIM_Matrix_Rank))
Textual_SIM_sparse=scipy.sparse.csc_matrix(Textual_SIM_matrix)   
np.savetxt("Textual_SIM.csv", Textual_SIM_matrix, delimiter=",")
save_sparse("scotus.sparse.textual_SIM", Textual_SIM_sparse);


